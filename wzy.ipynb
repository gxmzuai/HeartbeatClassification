{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "799aac3d-1ede-4844-afb4-e9cea23d294c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: pandas in ./miniconda3/lib/python3.8/site-packages (2.0.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./miniconda3/lib/python3.8/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in ./miniconda3/lib/python3.8/site-packages (from pandas) (1.22.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./miniconda3/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./miniconda3/lib/python3.8/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in ./miniconda3/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f209474-b87b-468f-9ff7-defc80d299be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: scikit-learn in ./miniconda3/lib/python3.8/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in ./miniconda3/lib/python3.8/site-packages (from scikit-learn) (1.22.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./miniconda3/lib/python3.8/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./miniconda3/lib/python3.8/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in ./miniconda3/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c7d071a-da8a-451d-aab8-8c9f733ce64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader \n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36935ea7-dd2d-4ea5-a6cd-46f28148486a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "440c1c6f-70ee-493a-8ce7-3a76fa2df75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>heartbeat_signals</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.9912297987616655,0.9435330436439665,0.764677...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9714822034884503,0.9289687459588268,0.572932...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0,0.9591487564065292,0.7013782792997189,0.23...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.9757952826275774,0.9340884687738161,0.659636...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0,0.055816398940721094,0.26129357194994196,0...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                  heartbeat_signals  label\n",
       "0   0  0.9912297987616655,0.9435330436439665,0.764677...    0.0\n",
       "1   1  0.9714822034884503,0.9289687459588268,0.572932...    0.0\n",
       "2   2  1.0,0.9591487564065292,0.7013782792997189,0.23...    2.0\n",
       "3   3  0.9757952826275774,0.9340884687738161,0.659636...    0.0\n",
       "4   4  0.0,0.055816398940721094,0.26129357194994196,0...    2.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\"train.csv\")\n",
    "data_test=pd.read_csv(\"testA.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f479ae75-f840-4816-a426-8734d84545a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9912297987616655</td>\n",
       "      <td>0.9435330436439665</td>\n",
       "      <td>0.7646772997256593</td>\n",
       "      <td>0.6185708990212999</td>\n",
       "      <td>0.3796321642826237</td>\n",
       "      <td>0.19082233510621885</td>\n",
       "      <td>0.040237131594430715</td>\n",
       "      <td>0.02599520771717858</td>\n",
       "      <td>0.03170886048677242</td>\n",
       "      <td>0.06552357497104398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.9714822034884503</td>\n",
       "      <td>0.9289687459588268</td>\n",
       "      <td>0.5729328050711678</td>\n",
       "      <td>0.1784566262750076</td>\n",
       "      <td>0.1229615224365985</td>\n",
       "      <td>0.13236021729815928</td>\n",
       "      <td>0.09439236984499814</td>\n",
       "      <td>0.08957535516351411</td>\n",
       "      <td>0.030480606866741047</td>\n",
       "      <td>0.04049936195430977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9591487564065292</td>\n",
       "      <td>0.7013782792997189</td>\n",
       "      <td>0.23177753487886463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.08069805776387916</td>\n",
       "      <td>0.12837603937503544</td>\n",
       "      <td>0.18744837555079963</td>\n",
       "      <td>0.28082571505275855</td>\n",
       "      <td>0.3282610568488903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.9757952826275774</td>\n",
       "      <td>0.9340884687738161</td>\n",
       "      <td>0.6596366611990001</td>\n",
       "      <td>0.2499208267606008</td>\n",
       "      <td>0.23711575621286213</td>\n",
       "      <td>0.28144491730834825</td>\n",
       "      <td>0.2499208267606008</td>\n",
       "      <td>0.2499208267606008</td>\n",
       "      <td>0.24139674778512604</td>\n",
       "      <td>0.2306703464848836</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055816398940721094</td>\n",
       "      <td>0.26129357194994196</td>\n",
       "      <td>0.35984696254197834</td>\n",
       "      <td>0.43314263962884686</td>\n",
       "      <td>0.45369772898632504</td>\n",
       "      <td>0.49900406742109477</td>\n",
       "      <td>0.5427959768500487</td>\n",
       "      <td>0.6169044962835193</td>\n",
       "      <td>0.6766958323316207</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.677705342021188</td>\n",
       "      <td>0.22239242747868546</td>\n",
       "      <td>0.2571578307224994</td>\n",
       "      <td>0.20469042415279454</td>\n",
       "      <td>0.05466497618736314</td>\n",
       "      <td>0.026152286890497062</td>\n",
       "      <td>0.11818142707296006</td>\n",
       "      <td>0.24483757081121627</td>\n",
       "      <td>0.3289485158861968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>0.9268571578157265</td>\n",
       "      <td>0.9063471198026871</td>\n",
       "      <td>0.6369932212888393</td>\n",
       "      <td>0.41503751002775946</td>\n",
       "      <td>0.37474480119929776</td>\n",
       "      <td>0.3825812845814957</td>\n",
       "      <td>0.35894293360916163</td>\n",
       "      <td>0.34135861850914284</td>\n",
       "      <td>0.3365254578264915</td>\n",
       "      <td>0.3170292884548231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>0.9258351628306013</td>\n",
       "      <td>0.5873839035878395</td>\n",
       "      <td>0.6332261741951388</td>\n",
       "      <td>0.6323533645350808</td>\n",
       "      <td>0.6392827243034813</td>\n",
       "      <td>0.6142923239940205</td>\n",
       "      <td>0.5991551019747257</td>\n",
       "      <td>0.5176324324889339</td>\n",
       "      <td>0.4038033525475481</td>\n",
       "      <td>0.2531748788594435</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9947621698382489</td>\n",
       "      <td>0.8297017704865509</td>\n",
       "      <td>0.45819277171637834</td>\n",
       "      <td>0.26416169623741237</td>\n",
       "      <td>0.24022845026183584</td>\n",
       "      <td>0.21376575735540573</td>\n",
       "      <td>0.18929103849637752</td>\n",
       "      <td>0.20381573166587716</td>\n",
       "      <td>0.21086610220048516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>0.9259994004527861</td>\n",
       "      <td>0.916476635326053</td>\n",
       "      <td>0.4042900774399834</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2630344094167657</td>\n",
       "      <td>0.3854310437765884</td>\n",
       "      <td>0.3610665021846972</td>\n",
       "      <td>0.33270794046870034</td>\n",
       "      <td>0.33985000288462475</td>\n",
       "      <td>0.3504972538285509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 205 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0                     1                    2    \\\n",
       "0      0.9912297987616655    0.9435330436439665   0.7646772997256593   \n",
       "1      0.9714822034884503    0.9289687459588268   0.5729328050711678   \n",
       "2                     1.0    0.9591487564065292   0.7013782792997189   \n",
       "3      0.9757952826275774    0.9340884687738161   0.6596366611990001   \n",
       "4                     0.0  0.055816398940721094  0.26129357194994196   \n",
       "...                   ...                   ...                  ...   \n",
       "99995                 1.0     0.677705342021188  0.22239242747868546   \n",
       "99996  0.9268571578157265    0.9063471198026871   0.6369932212888393   \n",
       "99997  0.9258351628306013    0.5873839035878395   0.6332261741951388   \n",
       "99998                 1.0    0.9947621698382489   0.8297017704865509   \n",
       "99999  0.9259994004527861     0.916476635326053   0.4042900774399834   \n",
       "\n",
       "                       3                    4                    5    \\\n",
       "0       0.6185708990212999   0.3796321642826237  0.19082233510621885   \n",
       "1       0.1784566262750076   0.1229615224365985  0.13236021729815928   \n",
       "2      0.23177753487886463                  0.0  0.08069805776387916   \n",
       "3       0.2499208267606008  0.23711575621286213  0.28144491730834825   \n",
       "4      0.35984696254197834  0.43314263962884686  0.45369772898632504   \n",
       "...                    ...                  ...                  ...   \n",
       "99995   0.2571578307224994  0.20469042415279454  0.05466497618736314   \n",
       "99996  0.41503751002775946  0.37474480119929776   0.3825812845814957   \n",
       "99997   0.6323533645350808   0.6392827243034813   0.6142923239940205   \n",
       "99998  0.45819277171637834  0.26416169623741237  0.24022845026183584   \n",
       "99999                  0.0   0.2630344094167657   0.3854310437765884   \n",
       "\n",
       "                        6                    7                     8    \\\n",
       "0      0.040237131594430715  0.02599520771717858   0.03170886048677242   \n",
       "1       0.09439236984499814  0.08957535516351411  0.030480606866741047   \n",
       "2       0.12837603937503544  0.18744837555079963   0.28082571505275855   \n",
       "3        0.2499208267606008   0.2499208267606008   0.24139674778512604   \n",
       "4       0.49900406742109477   0.5427959768500487    0.6169044962835193   \n",
       "...                     ...                  ...                   ...   \n",
       "99995  0.026152286890497062  0.11818142707296006   0.24483757081121627   \n",
       "99996   0.35894293360916163  0.34135861850914284    0.3365254578264915   \n",
       "99997    0.5991551019747257   0.5176324324889339    0.4038033525475481   \n",
       "99998   0.21376575735540573  0.18929103849637752   0.20381573166587716   \n",
       "99999    0.3610665021846972  0.33270794046870034   0.33985000288462475   \n",
       "\n",
       "                       9    ...  195  196  197  198  199  200  201  202  203  \\\n",
       "0      0.06552357497104398  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1      0.04049936195430977  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2       0.3282610568488903  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3       0.2306703464848836  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4       0.6766958323316207  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "...                    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "99995   0.3289485158861968  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "99996   0.3170292884548231  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "99997   0.2531748788594435  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "99998  0.21086610220048516  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "99999   0.3504972538285509  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "       204  \n",
       "0      0.0  \n",
       "1      0.0  \n",
       "2      0.0  \n",
       "3      0.0  \n",
       "4      0.0  \n",
       "...    ...  \n",
       "99995  0.0  \n",
       "99996  0.0  \n",
       "99997  0.0  \n",
       "99998  0.0  \n",
       "99999  0.0  \n",
       "\n",
       "[100000 rows x 205 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据处理\n",
    "data_1=data[\"heartbeat_signals\"].str.split(\",\", expand=True)\n",
    "data_test_1=data_test[\"heartbeat_signals\"].str.split(\",\", expand=True)\n",
    "data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd3518e6-43a3-48a7-9b31-ecd3ebd797ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 2., ..., 3., 2., 0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(data.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d76ed2e4-0eaa-471d-ac47-90f8b764e77c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.9912298 , 0.94353306, 0.7646773 , ..., 0.        ,\n",
       "         0.        , 0.        ]],\n",
       "\n",
       "       [[0.9714822 , 0.9289687 , 0.5729328 , ..., 0.        ,\n",
       "         0.        , 0.        ]],\n",
       "\n",
       "       [[1.        , 0.95914876, 0.7013783 , ..., 0.        ,\n",
       "         0.        , 0.        ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.9258352 , 0.5873839 , 0.63322616, ..., 0.        ,\n",
       "         0.        , 0.        ]],\n",
       "\n",
       "       [[1.        , 0.9947622 , 0.8297018 , ..., 0.        ,\n",
       "         0.        , 0.        ]],\n",
       "\n",
       "       [[0.9259994 , 0.9164766 , 0.40429008, ..., 0.        ,\n",
       "         0.        , 0.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2 = np.array(data_1).astype(\"float32\").reshape(-1,1,205)\n",
    "data_test_2= np.array(data_test_1).astype(\"float32\").reshape(-1,1,205)\n",
    "data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa29c500-fbd1-4504-b81b-35cd45d6d211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_816/3417645431.py:4: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  y_train=torch.tensor(data.label,dtype=int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9912298, 0.9435331, 0.7646773,  ..., 0.0000000, 0.0000000,\n",
       "          0.0000000]],\n",
       "\n",
       "        [[0.9714822, 0.9289687, 0.5729328,  ..., 0.0000000, 0.0000000,\n",
       "          0.0000000]],\n",
       "\n",
       "        [[1.0000000, 0.9591488, 0.7013783,  ..., 0.0000000, 0.0000000,\n",
       "          0.0000000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.9258352, 0.5873839, 0.6332262,  ..., 0.0000000, 0.0000000,\n",
       "          0.0000000]],\n",
       "\n",
       "        [[1.0000000, 0.9947622, 0.8297018,  ..., 0.0000000, 0.0000000,\n",
       "          0.0000000]],\n",
       "\n",
       "        [[0.9259994, 0.9164766, 0.4042901,  ..., 0.0000000, 0.0000000,\n",
       "          0.0000000]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_printoptions(precision=7)\n",
    "x_train=torch.tensor(data_2)\n",
    "x_test=torch.tensor(data_test_2)\n",
    "y_train=torch.tensor(data.label,dtype=int)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "282af89c-c7d3-42c9-a2a5-ebb6944154d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100000, 1, 205])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46d55f60-da5f-4aac-a14d-db7dd4ae02c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# 数据转移至GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "x_train=x_train.to(device)\n",
    "y_train=y_train.to(device)\n",
    "x_test=x_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e42d3c8-e4bc-49c1-b181-9ee73614c155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun  7 15:43:28 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:40:00.0 Off |                  N/A |\n",
      "| 47%   41C    P2    80W / 320W |   2073MiB / 10240MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2afb84d6-6744-4169-ae35-77ea99f6e00a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x7f7d40d37460>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = TensorDataset(x_train, y_train)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfe9ec4e-bebc-4dba-9d9e-c6b7e60e0c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f7cc8e1de80>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(x_test, batch_size=128, shuffle=True)\n",
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "790ddeb0-87b5-4307-9b55-07e47d272e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "782"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader) #DataLoader会将一个epoch按照bitch_size大小分为多少批次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4377dc6-c5f0-48cf-95ee-b7d4d70e3b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "class CNN_1(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN_1,self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels = 1,out_channels = 64,kernel_size = 7,stride = 1,padding = 3)\n",
    "        self.conv2 = nn.Conv1d(in_channels = 64,out_channels = 128,kernel_size = 5,stride = 1,padding = 2)\n",
    "        self.conv3 = nn.Conv1d(in_channels = 128,out_channels = 256,kernel_size = 3,stride = 1,padding = 1)\n",
    "        self.conv4 = nn.Conv1d(in_channels = 256,out_channels = 256,kernel_size = 3,stride = 1,padding = 1)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size = 2)\n",
    "        self.sleakyrelu = nn.LeakyReLU(negative_slope = 0.05)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.bn4 = nn.BatchNorm1d(256)\n",
    "        self.dropout=nn.Dropout(0.2)\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(6400, 4096),\n",
    "            nn.BatchNorm1d(4096),\n",
    "            nn.LeakyReLU(negative_slope = 0.05),\n",
    "            nn.Linear(4096, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(negative_slope = 0.05),\n",
    "            nn.Linear(64, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.bn1(self.conv1(x))\n",
    "        x = self.sleakyrelu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x = self.sleakyrelu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.bn3(self.conv3(x))\n",
    "        x = self.sleakyrelu(x)\n",
    "        x = self.bn4(self.conv4(x))\n",
    "        x = self.sleakyrelu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.flatten(x,start_dim=1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "class CNN_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_2, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 1,out_channels = 16,kernel_size = 11,stride =1 ,padding = 'same'), \n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.LeakyReLU(), \n",
    "            \n",
    "            nn.Conv1d(in_channels = 16,out_channels = 32,kernel_size = 7,stride = 1,padding = 'same'),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Conv1d(in_channels = 32,out_channels = 64,kernel_size = 5,stride = 1,padding = 1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(kernel_size = 3, stride = 2),\n",
    "            \n",
    "            nn.Conv1d(in_channels = 64,out_channels = 128,kernel_size = 3,stride = 1,padding = 1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Conv1d(in_channels = 128,out_channels = 128,kernel_size = 3,stride = 1,padding = 1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(kernel_size= 3, stride = 2),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(6400,4096),\n",
    "            nn.BatchNorm1d(4096),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(4096, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 4),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = torch.flatten(x,start_dim=1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02f37ec2-fcaf-43d7-b6e1-8031049b4557",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_channels, in_channels // reduction, bias=False)\n",
    "        self.fc2 = nn.Linear(in_channels // reduction, in_channels, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, num_channels, _ = x.size()\n",
    "        squeeze = x.mean(-1)\n",
    "        excitation = self.fc1(squeeze)\n",
    "        excitation = self.relu(excitation)\n",
    "        excitation = self.fc2(excitation)\n",
    "        excitation = self.sigmoid(excitation).view(batch_size, num_channels, 1)\n",
    "        return x * excitation\n",
    "\n",
    "class SENet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SENet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=7, stride=1, padding=3)\n",
    "        self.se1 = SEBlock(64)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=5, stride=1, padding=2)\n",
    "        self.se2 = SEBlock(128)\n",
    "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.se3 = SEBlock(256)\n",
    "        self.conv4 = nn.Conv1d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.se4 = SEBlock(256)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.leakyrelu = nn.LeakyReLU(negative_slope=0.05)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.bn4 = nn.BatchNorm1d(256)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(6400, 4096),\n",
    "            nn.BatchNorm1d(4096),\n",
    "            nn.LeakyReLU(negative_slope=0.05),\n",
    "            nn.Linear(4096, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(negative_slope=0.05),\n",
    "            nn.Linear(64, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(self.conv1(x))\n",
    "        x = self.se1(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x = self.se2(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.bn3(self.conv3(x))\n",
    "        x = self.se3(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        x = self.bn4(self.conv4(x))\n",
    "        x = self.se4(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.linear(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01f2b4ae-b450-4e71-934c-89b9ef91709a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train_loss: 3.73e-01\n",
      "Epoch: 2, train_loss: 1.84e-01\n",
      "Epoch: 3, train_loss: 1.35e-01\n",
      "Epoch: 4, train_loss: 1.05e-01\n",
      "Epoch: 5, train_loss: 8.53e-02\n",
      "Epoch: 6, train_loss: 6.98e-02\n",
      "Epoch: 7, train_loss: 5.81e-02\n",
      "Epoch: 8, train_loss: 4.85e-02\n",
      "Epoch: 9, train_loss: 4.08e-02\n",
      "Epoch: 10, train_loss: 3.51e-02\n",
      "Epoch: 11, train_loss: 2.99e-02\n",
      "Epoch: 12, train_loss: 2.61e-02\n",
      "Epoch: 13, train_loss: 2.25e-02\n",
      "Epoch: 14, train_loss: 1.97e-02\n",
      "Epoch: 15, train_loss: 1.71e-02\n",
      "Epoch: 16, train_loss: 1.48e-02\n",
      "Epoch: 17, train_loss: 1.33e-02\n",
      "Epoch: 18, train_loss: 1.19e-02\n",
      "Epoch: 19, train_loss: 1.06e-02\n",
      "Epoch: 20, train_loss: 9.62e-03\n",
      "Epoch: 21, train_loss: 8.59e-03\n",
      "Epoch: 22, train_loss: 7.63e-03\n",
      "Epoch: 23, train_loss: 6.73e-03\n",
      "Epoch: 24, train_loss: 6.14e-03\n",
      "Epoch: 25, train_loss: 5.52e-03\n",
      "Epoch: 26, train_loss: 5.34e-03\n",
      "Epoch: 27, train_loss: 5.00e-03\n",
      "Epoch: 28, train_loss: 4.35e-03\n",
      "Epoch: 29, train_loss: 4.02e-03\n",
      "Epoch: 30, train_loss: 3.79e-03\n",
      "Epoch: 31, train_loss: 3.62e-03\n",
      "Epoch: 32, train_loss: 3.04e-03\n",
      "Epoch: 33, train_loss: 2.79e-03\n",
      "Epoch: 34, train_loss: 2.70e-03\n",
      "Epoch: 35, train_loss: 2.75e-03\n",
      "Epoch: 36, train_loss: 2.52e-03\n",
      "Epoch: 37, train_loss: 2.30e-03\n",
      "Epoch: 38, train_loss: 2.22e-03\n",
      "Epoch: 39, train_loss: 1.70e-03\n",
      "Epoch: 40, train_loss: 2.09e-03\n",
      "Epoch: 41, train_loss: 2.11e-03\n",
      "Epoch: 42, train_loss: 1.79e-03\n",
      "Epoch: 43, train_loss: 1.58e-03\n",
      "Epoch: 44, train_loss: 1.46e-03\n",
      "Epoch: 45, train_loss: 1.64e-03\n",
      "Epoch: 46, train_loss: 1.64e-03\n",
      "Epoch: 47, train_loss: 1.27e-03\n",
      "Epoch: 48, train_loss: 1.18e-03\n",
      "Epoch: 49, train_loss: 1.24e-03\n",
      "Epoch: 50, train_loss: 1.17e-03\n",
      "Epoch: 51, train_loss: 1.09e-03\n",
      "Epoch: 52, train_loss: 1.44e-03\n",
      "Epoch: 53, train_loss: 1.03e-03\n",
      "Epoch: 54, train_loss: 8.90e-04\n",
      "Epoch: 55, train_loss: 1.09e-03\n",
      "Epoch: 56, train_loss: 9.83e-04\n",
      "Epoch: 57, train_loss: 8.39e-04\n",
      "Epoch: 58, train_loss: 7.12e-04\n",
      "Epoch: 59, train_loss: 8.55e-04\n",
      "Epoch: 60, train_loss: 9.40e-04\n",
      "Epoch: 61, train_loss: 6.06e-04\n",
      "Epoch: 62, train_loss: 1.02e-03\n",
      "Epoch: 63, train_loss: 6.12e-04\n",
      "Epoch: 64, train_loss: 1.00e-03\n",
      "Epoch: 65, train_loss: 1.06e-03\n",
      "Epoch: 66, train_loss: 6.96e-04\n",
      "Epoch: 67, train_loss: 5.97e-04\n",
      "Epoch: 68, train_loss: 6.11e-04\n",
      "Epoch: 69, train_loss: 7.94e-04\n",
      "Epoch: 70, train_loss: 6.06e-04\n",
      "Epoch: 71, train_loss: 5.30e-04\n",
      "Epoch: 72, train_loss: 4.77e-04\n",
      "Epoch: 73, train_loss: 5.97e-04\n",
      "Epoch: 74, train_loss: 6.05e-04\n",
      "Epoch: 75, train_loss: 4.01e-04\n",
      "Epoch: 76, train_loss: 7.37e-04\n",
      "Epoch: 77, train_loss: 7.33e-04\n",
      "Epoch: 78, train_loss: 6.01e-04\n",
      "Epoch: 79, train_loss: 5.54e-04\n",
      "Epoch: 80, train_loss: 5.42e-04\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn as nn\n",
    "\n",
    "# 初始化模型\n",
    "model_senet = SENet()\n",
    "\n",
    "# 权值初始化\n",
    "for m in model_senet.modules():\n",
    "    if isinstance(m, nn.Conv1d):\n",
    "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_senet.parameters(), lr=0.00001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "model_senet.to(device)\n",
    "\n",
    "num_epochs = 80\n",
    "early_stopping_patience = 10\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# 全训练集训练\n",
    "for epoch in range(num_epochs):\n",
    "    model_senet.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model_senet(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    print('Epoch: {}, train_loss: {:.2e}'.format(epoch + 1, train_loss))\n",
    "    \n",
    "    # 动态学习率调整\n",
    "    scheduler.step(train_loss)\n",
    "    \n",
    "    # 早停机制\n",
    "    if train_loss < best_loss:\n",
    "        best_loss = train_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping at epoch:\", epoch + 1)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63e5fd84-dd0f-46d8-9f15-0b6e07398fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train_loss: 2.43e-01\n",
      "Epoch: 2, train_loss: 1.22e-01\n",
      "Epoch: 3, train_loss: 9.22e-02\n",
      "Epoch: 4, train_loss: 7.46e-02\n",
      "Epoch: 5, train_loss: 6.21e-02\n",
      "Epoch: 6, train_loss: 5.19e-02\n",
      "Epoch: 7, train_loss: 4.51e-02\n",
      "Epoch: 8, train_loss: 3.85e-02\n",
      "Epoch: 9, train_loss: 3.32e-02\n",
      "Epoch: 10, train_loss: 2.92e-02\n",
      "Epoch: 11, train_loss: 2.56e-02\n",
      "Epoch: 12, train_loss: 2.25e-02\n",
      "Epoch: 13, train_loss: 1.97e-02\n",
      "Epoch: 14, train_loss: 1.76e-02\n",
      "Epoch: 15, train_loss: 1.58e-02\n",
      "Epoch: 16, train_loss: 1.40e-02\n",
      "Epoch: 17, train_loss: 1.26e-02\n",
      "Epoch: 18, train_loss: 1.08e-02\n",
      "Epoch: 19, train_loss: 1.01e-02\n",
      "Epoch: 20, train_loss: 9.23e-03\n",
      "Epoch: 21, train_loss: 7.96e-03\n",
      "Epoch: 22, train_loss: 7.54e-03\n",
      "Epoch: 23, train_loss: 6.67e-03\n",
      "Epoch: 24, train_loss: 6.03e-03\n",
      "Epoch: 25, train_loss: 5.85e-03\n",
      "Epoch: 26, train_loss: 5.24e-03\n",
      "Epoch: 27, train_loss: 5.02e-03\n",
      "Epoch: 28, train_loss: 4.51e-03\n",
      "Epoch: 29, train_loss: 3.95e-03\n",
      "Epoch: 30, train_loss: 3.81e-03\n",
      "Epoch: 31, train_loss: 3.45e-03\n",
      "Epoch: 32, train_loss: 3.26e-03\n",
      "Epoch: 33, train_loss: 3.21e-03\n",
      "Epoch: 34, train_loss: 2.85e-03\n",
      "Epoch: 35, train_loss: 2.49e-03\n",
      "Epoch: 36, train_loss: 2.66e-03\n",
      "Epoch: 37, train_loss: 2.78e-03\n",
      "Epoch: 38, train_loss: 1.97e-03\n",
      "Epoch: 39, train_loss: 2.04e-03\n",
      "Epoch: 40, train_loss: 1.77e-03\n",
      "Epoch: 41, train_loss: 2.00e-03\n",
      "Epoch: 42, train_loss: 1.94e-03\n",
      "Epoch: 43, train_loss: 1.80e-03\n",
      "Epoch: 44, train_loss: 1.35e-03\n",
      "Epoch: 45, train_loss: 1.61e-03\n",
      "Epoch: 46, train_loss: 1.41e-03\n",
      "Epoch: 47, train_loss: 1.44e-03\n",
      "Epoch: 48, train_loss: 1.40e-03\n",
      "Epoch: 49, train_loss: 1.24e-03\n",
      "Epoch: 50, train_loss: 1.14e-03\n",
      "Epoch: 51, train_loss: 1.35e-03\n",
      "Epoch: 52, train_loss: 1.16e-03\n",
      "Epoch: 53, train_loss: 9.79e-04\n",
      "Epoch: 54, train_loss: 9.58e-04\n",
      "Epoch: 55, train_loss: 9.64e-04\n",
      "Epoch: 56, train_loss: 1.13e-03\n",
      "Epoch: 57, train_loss: 1.06e-03\n",
      "Epoch: 58, train_loss: 1.18e-03\n",
      "Epoch: 59, train_loss: 5.79e-04\n",
      "Epoch: 60, train_loss: 6.64e-04\n",
      "Epoch: 61, train_loss: 7.15e-04\n",
      "Epoch: 62, train_loss: 8.33e-04\n",
      "Epoch: 63, train_loss: 6.94e-04\n",
      "Epoch: 64, train_loss: 7.94e-04\n",
      "Epoch: 65, train_loss: 9.64e-04\n",
      "Epoch 00065: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch: 66, train_loss: 5.64e-04\n",
      "Epoch: 67, train_loss: 4.38e-04\n",
      "Epoch: 68, train_loss: 4.19e-04\n",
      "Epoch: 69, train_loss: 3.24e-04\n",
      "Epoch: 70, train_loss: 3.47e-04\n",
      "Epoch: 71, train_loss: 3.10e-04\n",
      "Epoch: 72, train_loss: 3.12e-04\n",
      "Epoch: 73, train_loss: 3.22e-04\n",
      "Epoch: 74, train_loss: 2.97e-04\n",
      "Epoch: 75, train_loss: 2.71e-04\n",
      "Epoch: 76, train_loss: 2.82e-04\n",
      "Epoch: 77, train_loss: 2.73e-04\n",
      "Epoch: 78, train_loss: 2.76e-04\n",
      "Epoch: 79, train_loss: 2.79e-04\n",
      "Epoch: 80, train_loss: 2.49e-04\n",
      "Epoch: 81, train_loss: 2.82e-04\n",
      "Epoch: 82, train_loss: 2.48e-04\n",
      "Epoch: 83, train_loss: 2.32e-04\n",
      "Epoch: 84, train_loss: 2.42e-04\n",
      "Epoch: 85, train_loss: 2.29e-04\n",
      "Epoch: 86, train_loss: 2.18e-04\n",
      "Epoch: 87, train_loss: 2.06e-04\n",
      "Epoch: 88, train_loss: 2.15e-04\n",
      "Epoch: 89, train_loss: 2.15e-04\n",
      "Epoch: 90, train_loss: 2.14e-04\n",
      "Epoch: 91, train_loss: 2.01e-04\n",
      "Epoch: 92, train_loss: 2.15e-04\n",
      "Epoch: 93, train_loss: 1.92e-04\n",
      "Epoch: 94, train_loss: 1.86e-04\n",
      "Epoch: 95, train_loss: 1.96e-04\n",
      "Epoch: 96, train_loss: 1.97e-04\n",
      "Epoch: 97, train_loss: 1.94e-04\n",
      "Epoch: 98, train_loss: 1.81e-04\n",
      "Epoch: 99, train_loss: 1.88e-04\n",
      "Epoch: 100, train_loss: 1.65e-04\n",
      "Epoch: 101, train_loss: 1.85e-04\n",
      "Epoch: 102, train_loss: 1.69e-04\n",
      "Epoch: 103, train_loss: 1.84e-04\n",
      "Epoch: 104, train_loss: 1.66e-04\n",
      "Epoch: 105, train_loss: 1.76e-04\n",
      "Epoch: 106, train_loss: 1.67e-04\n",
      "Epoch 00106: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch: 107, train_loss: 1.67e-04\n",
      "Epoch: 108, train_loss: 1.55e-04\n",
      "Epoch: 109, train_loss: 1.46e-04\n",
      "Epoch: 110, train_loss: 1.57e-04\n",
      "Epoch: 111, train_loss: 1.60e-04\n",
      "Epoch: 112, train_loss: 1.45e-04\n",
      "Epoch: 113, train_loss: 1.58e-04\n",
      "Epoch: 114, train_loss: 1.40e-04\n",
      "Epoch: 115, train_loss: 1.50e-04\n",
      "Epoch: 116, train_loss: 1.51e-04\n",
      "Epoch: 117, train_loss: 1.41e-04\n",
      "Epoch: 118, train_loss: 1.47e-04\n",
      "Epoch: 119, train_loss: 1.47e-04\n",
      "Epoch: 120, train_loss: 1.52e-04\n",
      "Epoch 00120: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch: 121, train_loss: 1.46e-04\n",
      "Epoch: 122, train_loss: 1.53e-04\n",
      "Epoch: 123, train_loss: 1.48e-04\n",
      "Epoch: 124, train_loss: 1.50e-04\n",
      "Early stopping at epoch: 124\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn as nn\n",
    "\n",
    "# 初始化模型、损失函数和优化器\n",
    "model_1 = CNN_1()\n",
    "# model_2 = CNN_2()\n",
    "\n",
    "# 权值初始化\n",
    "for m in model_1.modules():\n",
    "    if isinstance(m, nn.Conv1d):\n",
    "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_1.parameters(), lr=0.00001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "model_1.to(device)\n",
    "\n",
    "num_epochs = 150\n",
    "early_stopping_patience = 10\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# 全训练集训练\n",
    "for epoch in range(num_epochs): \n",
    "    model_1.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader: \n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model_1(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    print('Epoch: {}, train_loss: {:.2e}'.format(epoch + 1, train_loss))\n",
    "    \n",
    "    # 动态学习率调整\n",
    "    scheduler.step(train_loss)\n",
    "    \n",
    "    # 早停机制\n",
    "    if train_loss < best_loss:\n",
    "        best_loss = train_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping at epoch:\", epoch + 1)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c71159f2-3ada-4cd0-8def-5757c45c6dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train_loss: 2.89e-01\n",
      "Epoch: 2, train_loss: 1.43e-01\n",
      "Epoch: 3, train_loss: 1.08e-01\n",
      "Epoch: 4, train_loss: 8.56e-02\n",
      "Epoch: 5, train_loss: 7.00e-02\n",
      "Epoch: 6, train_loss: 5.83e-02\n",
      "Epoch: 7, train_loss: 4.92e-02\n",
      "Epoch: 8, train_loss: 4.12e-02\n",
      "Epoch: 9, train_loss: 3.53e-02\n",
      "Epoch: 10, train_loss: 2.99e-02\n",
      "Epoch: 11, train_loss: 2.59e-02\n",
      "Epoch: 12, train_loss: 2.25e-02\n",
      "Epoch: 13, train_loss: 1.94e-02\n",
      "Epoch: 14, train_loss: 1.70e-02\n",
      "Epoch: 15, train_loss: 1.50e-02\n",
      "Epoch: 16, train_loss: 1.30e-02\n",
      "Epoch: 17, train_loss: 1.15e-02\n",
      "Epoch: 18, train_loss: 1.03e-02\n",
      "Epoch: 19, train_loss: 9.07e-03\n",
      "Epoch: 20, train_loss: 7.85e-03\n",
      "Epoch: 21, train_loss: 7.53e-03\n",
      "Epoch: 22, train_loss: 6.59e-03\n",
      "Epoch: 23, train_loss: 5.89e-03\n",
      "Epoch: 24, train_loss: 4.94e-03\n",
      "Epoch: 25, train_loss: 4.75e-03\n",
      "Epoch: 26, train_loss: 4.35e-03\n",
      "Epoch: 27, train_loss: 3.61e-03\n",
      "Epoch: 28, train_loss: 3.74e-03\n",
      "Epoch: 29, train_loss: 3.23e-03\n",
      "Epoch: 30, train_loss: 3.35e-03\n",
      "Epoch: 31, train_loss: 2.86e-03\n",
      "Epoch: 32, train_loss: 2.48e-03\n",
      "Epoch: 33, train_loss: 2.61e-03\n",
      "Epoch: 34, train_loss: 2.20e-03\n",
      "Epoch: 35, train_loss: 2.37e-03\n",
      "Epoch: 36, train_loss: 2.22e-03\n",
      "Epoch: 37, train_loss: 1.97e-03\n",
      "Epoch: 38, train_loss: 1.60e-03\n",
      "Epoch: 39, train_loss: 1.59e-03\n",
      "Epoch: 40, train_loss: 1.36e-03\n",
      "Epoch: 41, train_loss: 1.37e-03\n",
      "Epoch: 42, train_loss: 1.53e-03\n",
      "Epoch: 43, train_loss: 1.36e-03\n",
      "Epoch: 44, train_loss: 1.28e-03\n",
      "Epoch: 45, train_loss: 1.29e-03\n",
      "Epoch: 46, train_loss: 1.25e-03\n",
      "Epoch: 47, train_loss: 9.17e-04\n",
      "Epoch: 48, train_loss: 9.29e-04\n",
      "Epoch: 49, train_loss: 9.27e-04\n",
      "Epoch: 50, train_loss: 8.74e-04\n",
      "Epoch: 51, train_loss: 8.44e-04\n",
      "Epoch: 52, train_loss: 8.72e-04\n",
      "Epoch: 53, train_loss: 9.56e-04\n",
      "Epoch: 54, train_loss: 7.30e-04\n",
      "Epoch: 55, train_loss: 6.83e-04\n",
      "Epoch: 56, train_loss: 6.57e-04\n",
      "Epoch: 57, train_loss: 6.40e-04\n",
      "Epoch: 58, train_loss: 7.13e-04\n",
      "Epoch: 59, train_loss: 7.00e-04\n",
      "Epoch: 60, train_loss: 6.50e-04\n",
      "Epoch: 61, train_loss: 7.18e-04\n",
      "Epoch: 62, train_loss: 6.14e-04\n",
      "Epoch: 63, train_loss: 6.94e-04\n",
      "Epoch: 64, train_loss: 5.52e-04\n",
      "Epoch: 65, train_loss: 6.17e-04\n",
      "Epoch: 66, train_loss: 4.25e-04\n",
      "Epoch: 67, train_loss: 5.66e-04\n",
      "Epoch: 68, train_loss: 4.47e-04\n",
      "Epoch: 69, train_loss: 4.63e-04\n",
      "Epoch: 70, train_loss: 6.83e-04\n",
      "Epoch: 71, train_loss: 4.29e-04\n",
      "Epoch: 72, train_loss: 3.91e-04\n",
      "Epoch: 73, train_loss: 5.17e-04\n",
      "Epoch: 74, train_loss: 4.89e-04\n",
      "Epoch: 75, train_loss: 3.62e-04\n",
      "Epoch: 76, train_loss: 4.10e-04\n",
      "Epoch: 77, train_loss: 5.93e-04\n",
      "Epoch: 78, train_loss: 3.85e-04\n",
      "Epoch: 79, train_loss: 3.50e-04\n",
      "Epoch: 80, train_loss: 4.78e-04\n",
      "Epoch: 81, train_loss: 2.42e-04\n",
      "Epoch: 82, train_loss: 4.18e-04\n",
      "Epoch: 83, train_loss: 2.99e-04\n",
      "Epoch: 84, train_loss: 3.00e-04\n",
      "Epoch: 85, train_loss: 3.42e-04\n",
      "Epoch: 86, train_loss: 3.39e-04\n",
      "Epoch: 87, train_loss: 2.27e-04\n",
      "Epoch: 88, train_loss: 3.59e-04\n",
      "Epoch: 89, train_loss: 3.35e-04\n",
      "Epoch: 90, train_loss: 2.66e-04\n",
      "Epoch: 91, train_loss: 2.47e-04\n",
      "Epoch: 92, train_loss: 2.40e-04\n",
      "Epoch: 93, train_loss: 3.52e-04\n",
      "Epoch 00093: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch: 94, train_loss: 2.08e-04\n",
      "Epoch: 95, train_loss: 1.69e-04\n",
      "Epoch: 96, train_loss: 1.46e-04\n",
      "Epoch: 97, train_loss: 1.46e-04\n",
      "Epoch: 98, train_loss: 1.54e-04\n",
      "Epoch: 99, train_loss: 1.25e-04\n",
      "Epoch: 100, train_loss: 1.45e-04\n",
      "Epoch: 101, train_loss: 1.22e-04\n",
      "Epoch: 102, train_loss: 9.90e-05\n",
      "Epoch: 103, train_loss: 1.11e-04\n",
      "Epoch: 104, train_loss: 1.08e-04\n",
      "Epoch: 105, train_loss: 1.43e-04\n",
      "Epoch: 106, train_loss: 9.81e-05\n",
      "Epoch: 107, train_loss: 1.04e-04\n",
      "Epoch: 108, train_loss: 1.05e-04\n",
      "Epoch: 109, train_loss: 1.05e-04\n",
      "Epoch: 110, train_loss: 9.49e-05\n",
      "Epoch: 111, train_loss: 8.35e-05\n",
      "Epoch: 112, train_loss: 1.04e-04\n",
      "Epoch: 113, train_loss: 1.19e-04\n",
      "Epoch: 114, train_loss: 9.84e-05\n",
      "Epoch: 115, train_loss: 8.22e-05\n",
      "Epoch: 116, train_loss: 8.82e-05\n",
      "Epoch: 117, train_loss: 1.00e-04\n",
      "Epoch: 118, train_loss: 8.20e-05\n",
      "Epoch: 119, train_loss: 7.56e-05\n",
      "Epoch: 120, train_loss: 8.42e-05\n",
      "Epoch: 121, train_loss: 7.31e-05\n",
      "Epoch: 122, train_loss: 6.92e-05\n",
      "Epoch: 123, train_loss: 7.36e-05\n",
      "Epoch: 124, train_loss: 7.72e-05\n",
      "Epoch: 125, train_loss: 7.86e-05\n",
      "Epoch: 126, train_loss: 7.15e-05\n",
      "Epoch: 127, train_loss: 6.99e-05\n",
      "Epoch: 128, train_loss: 6.92e-05\n",
      "Epoch 00128: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch: 129, train_loss: 6.82e-05\n",
      "Epoch: 130, train_loss: 6.59e-05\n",
      "Epoch: 131, train_loss: 6.39e-05\n",
      "Epoch: 132, train_loss: 6.41e-05\n",
      "Epoch: 133, train_loss: 6.47e-05\n",
      "Epoch: 134, train_loss: 1.18e-04\n",
      "Epoch: 135, train_loss: 7.76e-05\n",
      "Epoch: 136, train_loss: 6.82e-05\n",
      "Epoch: 137, train_loss: 6.01e-05\n",
      "Epoch: 138, train_loss: 6.56e-05\n",
      "Epoch: 139, train_loss: 8.36e-05\n",
      "Epoch: 140, train_loss: 6.85e-05\n",
      "Epoch: 141, train_loss: 6.25e-05\n",
      "Epoch: 142, train_loss: 7.24e-05\n",
      "Epoch: 143, train_loss: 6.13e-05\n",
      "Epoch 00143: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch: 144, train_loss: 6.48e-05\n",
      "Epoch: 145, train_loss: 5.99e-05\n",
      "Epoch: 146, train_loss: 7.73e-05\n",
      "Epoch: 147, train_loss: 7.44e-05\n",
      "Epoch: 148, train_loss: 6.34e-05\n",
      "Epoch: 149, train_loss: 6.00e-05\n",
      "Epoch: 150, train_loss: 6.19e-05\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn as nn\n",
    "\n",
    "# 初始化模型、损失函数和优化器\n",
    "# model_1 = CNN_1()\n",
    "model_2 = CNN_2()\n",
    "\n",
    "# 权值初始化\n",
    "for m in model_2.modules():\n",
    "    if isinstance(m, nn.Conv1d):\n",
    "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_2.parameters(), lr=0.00001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "model_2.to(device)\n",
    "\n",
    "num_epochs = 150\n",
    "early_stopping_patience = 10\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# 全训练集训练\n",
    "for epoch in range(num_epochs): \n",
    "    model_2.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader: \n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model_2(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    print('Epoch: {}, train_loss: {:.2e}'.format(epoch + 1, train_loss))\n",
    "    \n",
    "    # 动态学习率调整\n",
    "    scheduler.step(train_loss)\n",
    "    \n",
    "    # 早停机制\n",
    "    if train_loss < best_loss:\n",
    "        best_loss = train_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping at epoch:\", epoch + 1)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea3d2d8-745c-4166-b783-32dc05d1fd77",
   "metadata": {},
   "source": [
    "## 下面没有加早停和动态学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74c9838a-ffa1-46d5-ae5f-88c72a04c34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train_loss: 2.01e-01\n",
      "Epoch: 2, train_loss: 2.46e-01\n",
      "Epoch: 3, train_loss: 3.38e-02\n",
      "Epoch: 4, train_loss: 1.00e-01\n",
      "Epoch: 5, train_loss: 1.86e-01\n",
      "Epoch: 6, train_loss: 1.90e-02\n",
      "Epoch: 7, train_loss: 5.07e-02\n",
      "Epoch: 8, train_loss: 3.84e-02\n",
      "Epoch: 9, train_loss: 5.82e-02\n",
      "Epoch: 10, train_loss: 7.14e-02\n",
      "Epoch: 11, train_loss: 1.86e-02\n",
      "Epoch: 12, train_loss: 4.12e-03\n",
      "Epoch: 13, train_loss: 1.26e-02\n",
      "Epoch: 14, train_loss: 1.35e-01\n",
      "Epoch: 15, train_loss: 1.51e-02\n",
      "Epoch: 16, train_loss: 2.57e-02\n",
      "Epoch: 17, train_loss: 1.53e-02\n",
      "Epoch: 18, train_loss: 7.82e-02\n",
      "Epoch: 19, train_loss: 1.88e-02\n",
      "Epoch: 20, train_loss: 3.55e-03\n",
      "Epoch: 21, train_loss: 5.43e-03\n",
      "Epoch: 22, train_loss: 7.61e-03\n",
      "Epoch: 23, train_loss: 5.14e-02\n",
      "Epoch: 24, train_loss: 7.47e-03\n",
      "Epoch: 25, train_loss: 2.22e-02\n",
      "Epoch: 26, train_loss: 3.59e-03\n",
      "Epoch: 27, train_loss: 2.50e-02\n",
      "Epoch: 28, train_loss: 9.79e-04\n",
      "Epoch: 29, train_loss: 1.13e-03\n",
      "Epoch: 30, train_loss: 5.27e-03\n",
      "Epoch: 31, train_loss: 6.35e-04\n",
      "Epoch: 32, train_loss: 4.89e-03\n",
      "Epoch: 33, train_loss: 1.87e-03\n",
      "Epoch: 34, train_loss: 1.22e-02\n",
      "Epoch: 35, train_loss: 3.41e-03\n",
      "Epoch: 36, train_loss: 1.90e-03\n",
      "Epoch: 37, train_loss: 3.34e-03\n",
      "Epoch: 38, train_loss: 2.89e-02\n",
      "Epoch: 39, train_loss: 1.25e-03\n",
      "Epoch: 40, train_loss: 1.53e-03\n",
      "Epoch: 41, train_loss: 1.11e-01\n",
      "Epoch: 42, train_loss: 2.49e-04\n",
      "Epoch: 43, train_loss: 5.24e-04\n",
      "Epoch: 44, train_loss: 1.68e-03\n",
      "Epoch: 45, train_loss: 8.14e-04\n",
      "Epoch: 46, train_loss: 7.58e-03\n",
      "Epoch: 47, train_loss: 2.68e-03\n",
      "Epoch: 48, train_loss: 7.13e-04\n",
      "Epoch: 49, train_loss: 5.93e-04\n",
      "Epoch: 50, train_loss: 1.09e-03\n"
     ]
    }
   ],
   "source": [
    "# import torch.optim as optim\n",
    "\n",
    "# # 初始化模型\n",
    "# model_senet = SENet()\n",
    "\n",
    "# # 权值初始化\n",
    "# for m in model_senet.modules():\n",
    "#     if isinstance(m, nn.Conv1d):\n",
    "#         nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "#     elif isinstance(m, nn.Linear):\n",
    "#         nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model_senet.parameters(), lr=0.00001)\n",
    "\n",
    "# model_senet.to(device)\n",
    "\n",
    "# num_epochs = 50\n",
    "\n",
    "# # 全训练集训练\n",
    "# for epoch in range(num_epochs):\n",
    "#     model_senet.train()\n",
    "#     running_loss = 0.0\n",
    "#     for inputs, targets in train_loader:\n",
    "#         inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "#         # 前向传播\n",
    "#         outputs = model_senet(inputs)\n",
    "#         loss = criterion(outputs, targets)\n",
    "        \n",
    "#         # 反向传播和优化\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         running_loss += loss.item()\n",
    "#     train_loss = running_loss / 100000\n",
    "    \n",
    "#     print('Epoch: {}, train_loss: {:.2e}'.format(epoch + 1, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60587d01-11d5-4097-8bc8-539d79cba786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1, train_loss: 7.07e-02\n",
      "Epoch:2, train_loss: 1.10e-01\n",
      "Epoch:3, train_loss: 5.07e-02\n",
      "Epoch:4, train_loss: 3.66e-02\n",
      "Epoch:5, train_loss: 4.21e-02\n",
      "Epoch:6, train_loss: 6.49e-02\n",
      "Epoch:7, train_loss: 1.74e-01\n",
      "Epoch:8, train_loss: 7.07e-02\n",
      "Epoch:9, train_loss: 3.30e-02\n",
      "Epoch:10, train_loss: 6.61e-02\n",
      "Epoch:11, train_loss: 3.11e-02\n",
      "Epoch:12, train_loss: 2.26e-02\n",
      "Epoch:13, train_loss: 9.85e-03\n",
      "Epoch:14, train_loss: 5.29e-02\n",
      "Epoch:15, train_loss: 2.90e-02\n",
      "Epoch:16, train_loss: 3.90e-03\n",
      "Epoch:17, train_loss: 3.93e-03\n",
      "Epoch:18, train_loss: 3.84e-02\n",
      "Epoch:19, train_loss: 8.19e-03\n",
      "Epoch:20, train_loss: 1.27e-02\n",
      "Epoch:21, train_loss: 1.09e-02\n",
      "Epoch:22, train_loss: 6.10e-03\n",
      "Epoch:23, train_loss: 2.25e-03\n",
      "Epoch:24, train_loss: 1.97e-03\n",
      "Epoch:25, train_loss: 8.31e-03\n",
      "Epoch:26, train_loss: 1.54e-02\n",
      "Epoch:27, train_loss: 3.39e-03\n",
      "Epoch:28, train_loss: 1.08e-02\n",
      "Epoch:29, train_loss: 1.68e-03\n",
      "Epoch:30, train_loss: 1.04e-02\n",
      "Epoch:31, train_loss: 3.06e-03\n",
      "Epoch:32, train_loss: 1.15e-02\n",
      "Epoch:33, train_loss: 5.73e-04\n",
      "Epoch:34, train_loss: 1.92e-03\n",
      "Epoch:35, train_loss: 1.26e-02\n",
      "Epoch:36, train_loss: 3.63e-03\n",
      "Epoch:37, train_loss: 1.59e-03\n",
      "Epoch:38, train_loss: 1.28e-02\n",
      "Epoch:39, train_loss: 3.46e-03\n",
      "Epoch:40, train_loss: 1.29e-03\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" model.eval()\\n    with torch.no_grad():\\n        val_loss = 0.0\\n        for val_inputs, val_targets in val_loader:\\n            val_outputs = model(val_inputs)\\n            val_loss += criterion(val_outputs, val_targets).item()\\n   \\n        val_loss /= len(dataset_2)\\n    \\n        print('Epoch: {}, Train Loss: {:.2e}, Val Loss: {:.2e}'.format(epoch+1, loss.item(), val_loss))\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # 初始化模型、损失函数和优化器\n",
    "\n",
    "# model_1 = CNN_1()\n",
    "# # model_2 = CNN_2()\n",
    "\n",
    "\n",
    "# #权值初始化\n",
    "# for m in model_1.modules():\n",
    "#     if isinstance(m, nn.Conv1d):\n",
    "#         nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "#     elif isinstance(m, nn.Linear):\n",
    "#         nn.init.xavier_normal_(m.weight)\n",
    "        \n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model_1.parameters(), lr=0.00001)\n",
    "\n",
    "# model_1.to(device)\n",
    "\n",
    "# num_epochs=40\n",
    "  \n",
    "# # 全训练集训练\n",
    "# for epoch in range(num_epochs): \n",
    "#     model_1.train()\n",
    "#     running_loss=0.0\n",
    "#     for inputs, targets in train_loader: \n",
    "        \n",
    "#         # 前向传播\n",
    "#         outputs = model_1(inputs)\n",
    "#         loss = criterion(outputs, targets)\n",
    "#         # 反向传播和优化\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step() \n",
    "#         running_loss += loss.item()\n",
    "#     train_loss = running_loss/100000\n",
    "    \n",
    "#     print('Epoch:{}, train_loss: {:.2e}'.format(epoch+1, loss.item()))\n",
    "    \n",
    "#     # 验证数据   \n",
    "# ''' model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         val_loss = 0.0\n",
    "#         for val_inputs, val_targets in val_loader:\n",
    "#             val_outputs = model(val_inputs)\n",
    "#             val_loss += criterion(val_outputs, val_targets).item()\n",
    "   \n",
    "#         val_loss /= len(dataset_2)\n",
    "    \n",
    "#         print('Epoch: {}, Train Loss: {:.2e}, Val Loss: {:.2e}'.format(epoch+1, loss.item(), val_loss))'''\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5daa7c1e-5b50-4df1-b465-d942aeaeeba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1, train_loss: 2.10e-01\n",
      "Epoch:2, train_loss: 1.75e-01\n",
      "Epoch:3, train_loss: 1.23e-01\n",
      "Epoch:4, train_loss: 6.57e-02\n",
      "Epoch:5, train_loss: 1.19e-01\n",
      "Epoch:6, train_loss: 3.98e-02\n",
      "Epoch:7, train_loss: 6.62e-02\n",
      "Epoch:8, train_loss: 5.45e-02\n",
      "Epoch:9, train_loss: 3.69e-02\n",
      "Epoch:10, train_loss: 1.81e-02\n",
      "Epoch:11, train_loss: 1.08e-02\n",
      "Epoch:12, train_loss: 1.99e-02\n",
      "Epoch:13, train_loss: 4.18e-02\n",
      "Epoch:14, train_loss: 1.96e-02\n",
      "Epoch:15, train_loss: 9.09e-03\n",
      "Epoch:16, train_loss: 7.63e-03\n",
      "Epoch:17, train_loss: 1.09e-02\n",
      "Epoch:18, train_loss: 8.02e-03\n",
      "Epoch:19, train_loss: 7.52e-03\n",
      "Epoch:20, train_loss: 4.56e-03\n",
      "Epoch:21, train_loss: 5.00e-03\n",
      "Epoch:22, train_loss: 7.90e-02\n",
      "Epoch:23, train_loss: 8.87e-03\n",
      "Epoch:24, train_loss: 1.01e-02\n",
      "Epoch:25, train_loss: 2.13e-03\n",
      "Epoch:26, train_loss: 5.87e-03\n",
      "Epoch:27, train_loss: 9.11e-03\n",
      "Epoch:28, train_loss: 7.11e-03\n",
      "Epoch:29, train_loss: 2.51e-03\n",
      "Epoch:30, train_loss: 1.74e-03\n",
      "Epoch:31, train_loss: 6.64e-04\n",
      "Epoch:32, train_loss: 9.62e-04\n",
      "Epoch:33, train_loss: 4.26e-03\n",
      "Epoch:34, train_loss: 1.74e-03\n",
      "Epoch:35, train_loss: 2.52e-02\n",
      "Epoch:36, train_loss: 1.39e-03\n",
      "Epoch:37, train_loss: 2.84e-03\n",
      "Epoch:38, train_loss: 1.31e-03\n",
      "Epoch:39, train_loss: 4.05e-04\n",
      "Epoch:40, train_loss: 1.21e-02\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" model.eval()\\n    with torch.no_grad():\\n        val_loss = 0.0\\n        for val_inputs, val_targets in val_loader:\\n            val_outputs = model(val_inputs)\\n            val_loss += criterion(val_outputs, val_targets).item()\\n   \\n        val_loss /= len(dataset_2)\\n    \\n        print('Epoch: {}, Train Loss: {:.2e}, Val Loss: {:.2e}'.format(epoch+1, loss.item(), val_loss))\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # 初始化模型、损失函数和优化器\n",
    "\n",
    "# # model_1 = CNN_1()\n",
    "# model_2 = CNN_2()\n",
    "\n",
    "\n",
    "# #权值初始化\n",
    "# for m in model_2.modules():\n",
    "#     if isinstance(m, nn.Conv1d):\n",
    "#         nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "#     elif isinstance(m, nn.Linear):\n",
    "#         nn.init.xavier_normal_(m.weight)\n",
    "        \n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model_2.parameters(), lr=0.00001)\n",
    "\n",
    "# model_2.to(device)\n",
    "\n",
    "# num_epochs=40\n",
    "  \n",
    "# # 全训练集训练\n",
    "# for epoch in range(num_epochs): \n",
    "#     model_2.train()\n",
    "#     running_loss=0.0\n",
    "#     for inputs, targets in train_loader: \n",
    "#         # 前向传播\n",
    "#         outputs = model_2(inputs)\n",
    "#         loss = criterion(outputs, targets)\n",
    "#         # 反向传播和优化\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step() \n",
    "#         running_loss += loss.item()\n",
    "#     train_loss = running_loss/100000\n",
    "    \n",
    "#     print('Epoch:{}, train_loss: {:.2e}'.format(epoch+1, loss.item()))\n",
    "    \n",
    "#     # 验证数据   \n",
    "# ''' model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         val_loss = 0.0\n",
    "#         for val_inputs, val_targets in val_loader:\n",
    "#             val_outputs = model(val_inputs)\n",
    "#             val_loss += criterion(val_outputs, val_targets).item()\n",
    "   \n",
    "#         val_loss /= len(dataset_2)\n",
    "    \n",
    "#         print('Epoch: {}, Train Loss: {:.2e}, Val Loss: {:.2e}'.format(epoch+1, loss.item(), val_loss))'''\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86e5ef7f-0668-44aa-bbd0-f116ee7d1491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "#torch.save(model_1, 'model_1.pkl') \n",
    "torch.save(model_2, 'model_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9fa369f7-1d7e-4dac-b20e-23f487a726ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 保存模型\n",
    "# torch.save(model_1, 'model_1.pkl') \n",
    "# torch.save(model_2, 'model_2.pkl')\n",
    "# torch.save(model_senet, 'model_senet.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e9f0a78-d547-4c4b-8694-0f63e09f19ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型预测\n",
    "#model_1.eval()\n",
    "model_2.eval()\n",
    "#model_senet.eval()\n",
    "with torch.no_grad():\n",
    "   # output_1 = model_1(x_test)\n",
    "    output_2 = model_2(x_test)\n",
    "   # output_3 = model_senet(x_test)\n",
    "#output_11 = torch.softmax(output_1,dim=1)   \n",
    "output_22 = torch.softmax(output_2,dim=1)  \n",
    "#output_33 = torch.softmax(output_3,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72868dc7-1f48-4a38-9a51-2d552e45cf45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c034074-d196-4c47-8e1b-6f611ce28061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9999881e-01, 1.3559438e-07, 5.1877515e-07, 5.6808000e-07],\n",
       "        [4.5929855e-07, 3.4132186e-06, 9.9999380e-01, 2.2540105e-06],\n",
       "        [3.3575652e-08, 4.6551227e-07, 1.0491611e-06, 9.9999845e-01],\n",
       "        ...,\n",
       "        [4.5325575e-04, 2.1240468e-05, 9.9951971e-01, 5.7744760e-06],\n",
       "        [9.9999225e-01, 3.5243220e-06, 2.5126205e-06, 1.7427968e-06],\n",
       "        [6.8015790e-01, 2.5351407e-04, 1.1900196e-02, 3.0768836e-01]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a39f0713-dadd-49b8-90f3-860aef791c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9999952e-01, 2.0184212e-08, 3.3679385e-07, 1.0298263e-07],\n",
       "        [2.5338358e-07, 2.5656545e-06, 9.9999404e-01, 3.1210839e-06],\n",
       "        [4.1027299e-08, 7.0204869e-08, 4.6939203e-07, 9.9999940e-01],\n",
       "        ...,\n",
       "        [3.8767241e-05, 2.4874469e-06, 9.9994969e-01, 9.1163611e-06],\n",
       "        [9.9999547e-01, 1.3343224e-06, 2.1217863e-06, 1.0328185e-06],\n",
       "        [7.6255846e-01, 6.1165687e-05, 4.2383187e-02, 1.9499725e-01]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c7e4ec52-e80a-4ed4-9431-df6d2a391298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型融合\n",
    "# predict = 0.5 * output_11 + 0.5 * output_22 \n",
    "# predict\n",
    "# predict = (output_11 + output_22 + output_33)/3 \n",
    "# predict\n",
    "predict = output_22 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "967d8d6c-d909-4221-b499-80eb29cb6536",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff706650-5d2e-4279-98c0-18247a838821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9999995 , 0.00000002, 0.00000034, 0.0000001 ],\n",
       "       [0.00000025, 0.00000257, 0.99999404, 0.00000312],\n",
       "       [0.00000004, 0.00000007, 0.00000047, 0.9999994 ],\n",
       "       ...,\n",
       "       [0.00003877, 0.00000249, 0.9999497 , 0.00000912],\n",
       "       [0.99999547, 0.00000133, 0.00000212, 0.00000103],\n",
       "       [0.76255846, 0.00006117, 0.04238319, 0.19499725]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_cpu = predict.cpu().numpy()\n",
    "predict_cpu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "807fa06a-ab2c-4c39-b409-e33aeb4b7051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99999917, 0.00000008, 0.00000043, 0.00000034],\n",
       "       [0.00000036, 0.00000299, 0.9999939 , 0.00000269],\n",
       "       [0.00000004, 0.00000027, 0.00000076, 0.9999989 ],\n",
       "       ...,\n",
       "       [0.00024601, 0.00001186, 0.9997347 , 0.00000745],\n",
       "       [0.99999386, 0.00000243, 0.00000232, 0.00000139],\n",
       "       [0.7213582 , 0.00015734, 0.02714169, 0.2513428 ]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 预测概率低于0.5的样本处理\n",
    "numpy_array = predict_cpu \n",
    "for i in range(numpy_array.shape[0]):\n",
    "    row = numpy_array[i]\n",
    "    max_val = np.max(row)\n",
    "    sorted_row = np.sort(row)\n",
    "    second_max_val = sorted_row[-2]\n",
    "    if max_val <= 0.5:\n",
    "        if max_val - second_max_val >= 0.049:\n",
    "            row[row != max_val] = 0\n",
    "            row[row == max_val] = 1\n",
    "            numpy_array[i] = row\n",
    "        else:\n",
    "            min_val = np.min(row)\n",
    "            second_min_val = np.min(row[row != min_val])\n",
    "            row[row == min_val] = 0\n",
    "            row[row == second_min_val] = 0\n",
    "            numpy_array[i] = row\n",
    "numpy_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93006b59-afa6-4105-9848-802ac99e4faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_0</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>9.999992e-01</td>\n",
       "      <td>7.788930e-08</td>\n",
       "      <td>4.277845e-07</td>\n",
       "      <td>3.355313e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100001</th>\n",
       "      <td>3.563410e-07</td>\n",
       "      <td>2.989437e-06</td>\n",
       "      <td>9.999939e-01</td>\n",
       "      <td>2.687547e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100002</th>\n",
       "      <td>3.730148e-08</td>\n",
       "      <td>2.678586e-07</td>\n",
       "      <td>7.592766e-07</td>\n",
       "      <td>9.999989e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100003</th>\n",
       "      <td>9.999948e-01</td>\n",
       "      <td>1.566412e-06</td>\n",
       "      <td>1.588755e-06</td>\n",
       "      <td>2.017716e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100004</th>\n",
       "      <td>9.999650e-01</td>\n",
       "      <td>1.368527e-05</td>\n",
       "      <td>1.328572e-05</td>\n",
       "      <td>7.986689e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>9.999503e-01</td>\n",
       "      <td>7.238275e-06</td>\n",
       "      <td>3.629479e-05</td>\n",
       "      <td>6.169547e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>9.999979e-01</td>\n",
       "      <td>9.820301e-07</td>\n",
       "      <td>7.095744e-07</td>\n",
       "      <td>4.126110e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>2.460115e-04</td>\n",
       "      <td>1.186396e-05</td>\n",
       "      <td>9.997347e-01</td>\n",
       "      <td>7.445418e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>9.999939e-01</td>\n",
       "      <td>2.429322e-06</td>\n",
       "      <td>2.317203e-06</td>\n",
       "      <td>1.387808e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>7.213582e-01</td>\n",
       "      <td>1.573399e-04</td>\n",
       "      <td>2.714169e-02</td>\n",
       "      <td>2.513428e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             label_0       label_1       label_2       label_3\n",
       "100000  9.999992e-01  7.788930e-08  4.277845e-07  3.355313e-07\n",
       "100001  3.563410e-07  2.989437e-06  9.999939e-01  2.687547e-06\n",
       "100002  3.730148e-08  2.678586e-07  7.592766e-07  9.999989e-01\n",
       "100003  9.999948e-01  1.566412e-06  1.588755e-06  2.017716e-06\n",
       "100004  9.999650e-01  1.368527e-05  1.328572e-05  7.986689e-06\n",
       "...              ...           ...           ...           ...\n",
       "119995  9.999503e-01  7.238275e-06  3.629479e-05  6.169547e-06\n",
       "119996  9.999979e-01  9.820301e-07  7.095744e-07  4.126110e-07\n",
       "119997  2.460115e-04  1.186396e-05  9.997347e-01  7.445418e-06\n",
       "119998  9.999939e-01  2.429322e-06  2.317203e-06  1.387808e-06\n",
       "119999  7.213582e-01  1.573399e-04  2.714169e-02  2.513428e-01\n",
       "\n",
       "[20000 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "numpy_array_1=pd.DataFrame(numpy_array)\n",
    "numpy_array_1.columns=[\"label_0\",\"label_1\",\"label_2\",\"label_3\"]\n",
    "numpy_array_1.index=list(range(100000,120000))\n",
    "numpy_array_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2defc70b-974c-4b80-92fb-0836fd4c0867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.78893e-08"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_array_1.loc[100000,\"label_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5a5e18c-1f8b-459f-afa1-398093913a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_array_1.to_csv(\"test44.csv\",index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6ce305-5213-4b5a-accf-1c147445b596",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
